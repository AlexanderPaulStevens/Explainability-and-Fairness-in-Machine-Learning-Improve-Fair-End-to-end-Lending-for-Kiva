{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q8JYD36CdYr"
   },
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "_Qgy7Jmr5wSx",
    "outputId": "e43cd01b-9ccb-45ac-f21e-f8691c26d5e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_non_alphanum\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMknjdZpCgR0"
   },
   "source": [
    "## Get the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The data consists of two samples each having 10000 instances. \n",
    " - The first sample are the funded loans (bin 1) and the second sample are the expired loans (bin 2). \n",
    " - The loans are made between 2012 and 2017 and are distributed with a field partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_Oy1nXa6dLi"
   },
   "outputs": [],
   "source": [
    "combined= pd.read_csv(r\"sample_tfidf_corrected.csv\",index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "HVpq1ScSAOjc",
    "outputId": "b7faa54e-5da2-44ed-d6ec-aca7ad6437e6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>description_ENG</th>\n",
       "      <th>status</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>funded_amount</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_name</th>\n",
       "      <th>gender_reclassified</th>\n",
       "      <th>borrower_count</th>\n",
       "      <th>loan_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1382091</td>\n",
       "      <td>Carlos studied up to high school. He did not m...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>675.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Carlos Geovanny</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1101771</td>\n",
       "      <td>Edwin, 37 years old, lives with his wife and h...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Edwin Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>893806</td>\n",
       "      <td>Ana, age 34, lives with her life partner and t...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>450.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>ANA DOLORES</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>875440</td>\n",
       "      <td>Featured in the above picture is Peter, who ha...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>Peter's Group</td>\n",
       "      <td>male</td>\n",
       "      <td>12</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>813862</td>\n",
       "      <td>Qurbongul is a resident of the Khuroson distri...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Qurbongul</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1396192</td>\n",
       "      <td>Colman is the group leader of his group in Kil...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>75.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>Colman's Group</td>\n",
       "      <td>male</td>\n",
       "      <td>7</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>999018</td>\n",
       "      <td>Santos is 44 years old and sells car parts. He...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>425.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>SANTOS</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1034599</td>\n",
       "      <td>Bright is a 27 year old hardworking married ma...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>475.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>Bright</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1391541</td>\n",
       "      <td>Ashi is a 36-year-old married woman and the mo...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>175.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Ashi</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>578104</td>\n",
       "      <td>Omar is a 52-year-old father of five children....</td>\n",
       "      <td>expired</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>825.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Omar</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1254694</td>\n",
       "      <td>Santos live in Tegucigalpa, the capital of Hon...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>625.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>Santos</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1411569</td>\n",
       "      <td>Meet Gevorg from Gyumri where he lives with hi...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>Gevorg</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>766370</td>\n",
       "      <td>Marianito is a 62-year-old man who has worked ...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>225.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>Marianito</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1181205</td>\n",
       "      <td>Yaya was born in Mali and is 42 years old. Her...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>3175.0</td>\n",
       "      <td>Yiriwa Group</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>913116</td>\n",
       "      <td>Elmira is 48 years old, married, and has four ...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>Elmira</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>759215</td>\n",
       "      <td>Yayi lives with his wife and their daughter in...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>Yayi</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>564025</td>\n",
       "      <td>Francisco is the proud owner of Covering Ameri...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>1275.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>Francisco</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>895780</td>\n",
       "      <td>The ten members of the group “KOTOGNOGONTALA 2...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>Kotognogontala 2 Group</td>\n",
       "      <td>female</td>\n",
       "      <td>10</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>568198</td>\n",
       "      <td>Morís is 45 years old and lives in his private...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>575.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Moris</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1331068</td>\n",
       "      <td>Francisco is 69 years old, single and is dedic...</td>\n",
       "      <td>expired</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>FRANCISCO</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loan_id                                    description_ENG   status  year  \\\n",
       "0   1382091  Carlos studied up to high school. He did not m...  expired  2017   \n",
       "1   1101771  Edwin, 37 years old, lives with his wife and h...  expired  2016   \n",
       "2    893806  Ana, age 34, lives with her life partner and t...  expired  2015   \n",
       "3    875440  Featured in the above picture is Peter, who ha...  expired  2015   \n",
       "4    813862  Qurbongul is a resident of the Khuroson distri...  expired  2014   \n",
       "5   1396192  Colman is the group leader of his group in Kil...  expired  2017   \n",
       "6    999018  Santos is 44 years old and sells car parts. He...  expired  2015   \n",
       "7   1034599  Bright is a 27 year old hardworking married ma...  expired  2016   \n",
       "8   1391541  Ashi is a 36-year-old married woman and the mo...  expired  2017   \n",
       "9    578104  Omar is a 52-year-old father of five children....  expired  2013   \n",
       "10  1254694  Santos live in Tegucigalpa, the capital of Hon...  expired  2017   \n",
       "11  1411569  Meet Gevorg from Gyumri where he lives with hi...  expired  2017   \n",
       "12   766370  Marianito is a 62-year-old man who has worked ...  expired  2014   \n",
       "13  1181205  Yaya was born in Mali and is 42 years old. Her...  expired  2016   \n",
       "14   913116  Elmira is 48 years old, married, and has four ...  expired  2015   \n",
       "15   759215  Yayi lives with his wife and their daughter in...  expired  2014   \n",
       "16   564025  Francisco is the proud owner of Covering Ameri...  expired  2013   \n",
       "17   895780  The ten members of the group “KOTOGNOGONTALA 2...  expired  2015   \n",
       "18   568198  Morís is 45 years old and lives in his private...  expired  2013   \n",
       "19  1331068  Francisco is 69 years old, single and is dedic...  expired  2017   \n",
       "\n",
       "    month  funded_amount  loan_amount               loan_name  \\\n",
       "0       9          675.0       1000.0         Carlos Geovanny   \n",
       "1       6          100.0        400.0            Edwin Victor   \n",
       "2       5          450.0        800.0             ANA DOLORES   \n",
       "3       4          800.0       1250.0           Peter's Group   \n",
       "4      12         1200.0       1950.0               Qurbongul   \n",
       "5      10           75.0        550.0          Colman's Group   \n",
       "6      12          425.0       1300.0                  SANTOS   \n",
       "7       3          475.0        750.0                  Bright   \n",
       "8      10          175.0        400.0                    Ashi   \n",
       "9       6          825.0       2000.0                    Omar   \n",
       "10      3          625.0        875.0                  Santos   \n",
       "11     11         1625.0       2075.0                  Gevorg   \n",
       "12      9          225.0        925.0               Marianito   \n",
       "13     11         2950.0       3175.0            Yiriwa Group   \n",
       "14      7          275.0       1150.0                  Elmira   \n",
       "15      8          800.0       1050.0                    Yayi   \n",
       "16      5         1275.0       5450.0               Francisco   \n",
       "17      6          350.0       1100.0  Kotognogontala 2 Group   \n",
       "18      6          575.0       1000.0                   Moris   \n",
       "19      6         1025.0       1325.0              FRANCISCO    \n",
       "\n",
       "   gender_reclassified  borrower_count   loan_type  \n",
       "0                 male               1  individual  \n",
       "1                 male               1  individual  \n",
       "2               female               1  individual  \n",
       "3                 male              12       group  \n",
       "4               female               1  individual  \n",
       "5                 male               7       group  \n",
       "6                 male               1  individual  \n",
       "7                 male               1  individual  \n",
       "8               female               1  individual  \n",
       "9                 male               1  individual  \n",
       "10                male               1  individual  \n",
       "11                male               1  individual  \n",
       "12                male               1  individual  \n",
       "13                male               8       group  \n",
       "14              female               1  individual  \n",
       "15                male               1  individual  \n",
       "16                male               1  individual  \n",
       "17              female              10       group  \n",
       "18                male               1  individual  \n",
       "19                male               1  individual  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module performs some preprocessing steps to clean the descriptions. The following steps are used:\n",
    "   * remove special characters (e.g. \\\\r\\\\n, <br //>, \", (.*\\), -, ...)\n",
    "   * lowercase\n",
    "   * remove HTML code\n",
    "   * remove accents on words with unidecode\n",
    "   * remove names of borrowers (currently not used)\n",
    "   * lemmatization\n",
    "   * remove stopwords\n",
    "   * remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "combined['description_ENG_Parsed_1'] = combined['description_ENG']\n",
    "\n",
    "for i in range(len(combined)):\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG'][i].replace(\"\\\\r\\\\n\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"\\\\n\\\\n\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"\\\\n\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"\\\\r\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"<br>\", \"  \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"<br />\", \"  \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"<br /><br />\", \"  \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"<br><br>\", \"  \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"<br/><br/>\", \"  \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"-\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"\\\\t\", \" \")\n",
    "    combined['description_ENG_Parsed_1'][i] = combined['description_ENG_Parsed_1'][i].replace(\"'\",\" \")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_1']\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special characters\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace('\"', '')\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace('“', '')\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace('”', '')\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace(r'\\(.*\\)', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML code\n",
    "from bs4 import BeautifulSoup\n",
    "combined['description_ENG_Parsed_2'] = [BeautifulSoup(text, 'html.parser').get_text() for text in combined['description_ENG_Parsed_2'] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Unidecode\n",
    "# Remove apostrophe and accents\n",
    "from unidecode import unidecode\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace(\"'s\", \"\")\n",
    "combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].apply(unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "for punct_sign in punctuation_signs:\n",
    "    combined['description_ENG_Parsed_2'] = combined['description_ENG_Parsed_2'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove names of borrowers from descriptions\n",
    "\n",
    "#remove NaNs\n",
    "df = combined.copy()\n",
    "df = df.dropna(subset=['loan_name'])\n",
    "df = df[~df['loan_name'].str.contains(\"Group\")]\n",
    "df = df[~df['loan_name'].str.contains(\"group\")]\n",
    "#lowercase\n",
    "df['loan_name_processed'] = df['loan_name'].str.lower()\n",
    "#remove words which have less than 3 characters\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(r'\\b(\\w{1,2})\\b','')\n",
    "#set unicode\n",
    "df['loan_name_processed'] = df['loan_name_processed'].apply(unidecode)\n",
    "#remove ' and backslash\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(\"\\\\\", \"\")\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(\"'\", \"\")\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(\"–\", \"\")\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(r'\\(.*\\)', \"\")\n",
    "df['loan_name_processed'] = df['loan_name_processed'].str.replace(\".\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "#tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['loan_name_processed_2'] = df.apply(lambda row: nltk.word_tokenize(row['loan_name_processed']), axis=1)\n",
    "#remove words which have less than 3 characters\n",
    "df['loan_name_processed_2'] = df['loan_name_processed_2'].replace(r'\\b(\\w{1,2})\\b', '')\n",
    "\n",
    "\n",
    "def remove_words_from_text_body(row):\n",
    "    # Seperate the words to remove by the space between them\n",
    "    words_to_remove = row['loan_name_processed'].split(\" \")\n",
    "\n",
    "    # Get the text_body as a starting template\n",
    "    text_body = row['description_ENG_Parsed_2']\n",
    "\n",
    "    # For each word that we want to remove, replace it with \"\" (blank)\n",
    "    for word in words_to_remove:\n",
    "        text_body = text_body.replace(word, \"\")\n",
    "\n",
    "    return text_body\n",
    "\n",
    "df['description_ENG_Parsed_3'] = df.apply(remove_words_from_text_body, axis=1)\n",
    "combined['description_ENG_Parsed_3'] = df['description_ENG_Parsed_3'].copy()\n",
    "combined['description_ENG_Parsed_3']=combined['description_ENG_Parsed_3'].combine_first(combined['description_ENG_Parsed_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nrows = len(combined)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = combined.loc[row]['description_ENG_Parsed_2']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "combined['description_ENG_Parsed_4'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "newStopWords = ['years','year','to','be','and','a','are','the','in','of','have','for','with','also',\n",
    "               'business','loan','children','family','buy','work','sell','live','old','marry',\n",
    "                'income','request','help','use','future','earn','like','support','make','purchase','need',\n",
    "               'school','farm','save','husband','provide','able','hard','hop','improve','want','expand',\n",
    "                'one','store','increase','would','dream','house','pay','group','good','time','profit',\n",
    "                'home','enough','order','kiva','run','grow','products','food','customers','raise','life',\n",
    "                'fee','build','get','better','expense','new','market','well','day','start','small','clothe',\n",
    "                'him','community','village','continue','take','items','partner','stock','general','age',\n",
    "                'meet','plan','area','apply','capital','go','supply','education','farmer','shop','since',\n",
    "                'financial','operate','quality','ask','access','child','amount','household','sales','repay',\n",
    "                'service','send','thank','challenge']\n",
    "noise_words= ['rd','th','le','p','f','et','u','rs','fe','ue','u','e','f','m','b','st','nd','ly','s','al','la','ger'\n",
    "              ,'am','pm','etc','et','ms','en','de','el','la','lan','are']\n",
    "stop_words.extend(noise_words)\n",
    "#stop_words.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteStopWords = ['he','him','his','himself','she',\"she's\",'her','hers','herself']\n",
    "for i in deleteStopWords:\n",
    "    if i in stop_words:\n",
    "        stop_words.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# transforming numerical terms to digits\n",
    "\n",
    "combined['description_ENG_Parsed_7'] = combined['description_ENG_Parsed_4'].copy()\n",
    "from text_to_num import alpha2digit\n",
    "for i in range(len(combined)):\n",
    "    combined['description_ENG_Parsed_7'][i] = alpha2digit(combined['description_ENG_Parsed_7'][i], \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits and other things\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_7'].str.replace('\\d+', '', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace(\"'\",'', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('[','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('[','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace(']','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('\\\\',' ', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('\\\\',' ', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].map(lambda x: x.lstrip('\\\\ue').rstrip('aAbBcC'))\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('\\\\','')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].map(lambda x: x.lstrip(r\"\\ub\\ub\").rstrip('aAbBcC'))\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('*','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('+','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('(','', regex=True)\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace(')','', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "combined['description_ENG_Parsed_9'] = combined['description_ENG_Parsed_8'].copy()\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    combined['description_ENG_Parsed_9'] = combined['description_ENG_Parsed_9'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group similar words with word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [row.split() for row in combined['description_ENG_Parsed_9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averageLen(sentences) is 72.6771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderstevens/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3674"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist3= list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in wordlist:\n",
    "    if (string != \"\"):\n",
    "        wordlist3.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3674"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(word, list):\n",
    "    if word in list:\n",
    "        print(\"The word is in the list!\")\n",
    "    else:\n",
    "        print(\"The word is not in the list!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is not in the list!\n"
     ]
    }
   ],
   "source": [
    "check('le',wordlist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1878"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined[combined['description_ENG_Parsed_9'].str.contains(\" philippines \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output   continue able\n",
      "0.9060978\n",
      "output   old age\n",
      "0.9363674\n",
      "output   old marry\n",
      "0.96767807\n",
      "output   buy purchase\n",
      "0.93079257\n",
      "output   quality better\n",
      "0.9366164\n",
      "output   like would\n",
      "0.9463899\n",
      "output   purchase buy\n",
      "0.93079257\n",
      "output   able continue\n",
      "0.9060978\n",
      "output   age old\n",
      "0.9363674\n",
      "output   age marry\n",
      "0.9356429\n",
      "output   age mother\n",
      "0.9408483\n",
      "output   request php\n",
      "0.9086565\n",
      "output   marry old\n",
      "0.96767807\n",
      "output   marry age\n",
      "0.9356429\n",
      "output   expand future\n",
      "0.9023411\n",
      "output   mother age\n",
      "0.9408483\n",
      "output   would like\n",
      "0.9463899\n",
      "output   future expand\n",
      "0.9023411\n",
      "output   better quality\n",
      "0.9366164\n",
      "output   enough money\n",
      "0.93115366\n",
      "output   enough save\n",
      "0.9056247\n",
      "output   money enough\n",
      "0.93115366\n",
      "output   money save\n",
      "0.9516916\n",
      "output   save enough\n",
      "0.9056247\n",
      "output   save money\n",
      "0.9516916\n",
      "output   php request\n",
      "0.9086565\n",
      "output   fit art\n",
      "0.9268811\n",
      "output   fit arm\n",
      "0.90133077\n",
      "output   fit ease\n",
      "0.9120198\n",
      "output   art fit\n",
      "0.9268811\n",
      "output   arm fit\n",
      "0.90133077\n",
      "output   arm prove\n",
      "0.9261149\n",
      "output   arm row\n",
      "0.9331939\n",
      "output   prove arm\n",
      "0.9261149\n",
      "output   prove ease\n",
      "0.91123295\n",
      "output   ease fit\n",
      "0.9120198\n",
      "output   ease prove\n",
      "0.91123295\n",
      "output   row arm\n",
      "0.9331939\n"
     ]
    }
   ],
   "source": [
    "for i in wordlist3:\n",
    "    if len(combined[combined['description_ENG_Parsed_9'].str.contains(str(\"\")+i+str(\" \"))]) != False:\n",
    "        if combined['description_ENG_Parsed_9'].str.contains(str(\"\")+i+str(\" \")).value_counts()[True]>2000:\n",
    "            for j in wordlist3:\n",
    "                if len(combined[combined['description_ENG_Parsed_9'].str.contains(str(\"\")+j+str(\" \"))]) != False:\n",
    "                    if combined['description_ENG_Parsed_9'].str.contains(str(\"\")+j+str(\" \")).value_counts()[True]>2000:\n",
    "                        if i!=j:\n",
    "                            if w2v_model.wv.similarity(i,j)>0.90:\n",
    "                                print('output',' ',i,j)\n",
    "                                print(w2v_model.wv.similarity(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8893805"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('she','her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('his','he')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('him','he')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('children','child')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('job','work')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('his','he')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('purchase','buy')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('farm','farmer')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('dream','goal')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('provide','give')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('better','improve')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('her','she')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('request','ask')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('grateful','thank')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('learn','experience')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('ask','request')\n",
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('house','home')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6220141"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['description_ENG_Parsed_8'] = combined['description_ENG_Parsed_8'].str.replace('php','philippines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['description_ENG_Parsed_8'].str.contains('he').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjmrfd38CvTZ"
   },
   "source": [
    "# TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_codes = {\n",
    "    'funded': 1,\n",
    "    'expired': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping\n",
    "combined['status_Code'] = combined['status']\n",
    "combined = combined.replace({'status_Code':status_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['status_Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined['description_ENG_Parsed_8'], \n",
    "                                                    combined['status_Code'], \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 0.7\n",
    "#1. = 100%\n",
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gEh_ZzeCHx9"
   },
   "outputs": [],
   "source": [
    "#TF-IDF \n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True,\n",
    "                        strip_accents='unicode')\n",
    "\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "features_train2 = tfidf.fit_transform(X_train)\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "features_test2 = tfidf.transform(X_test)\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D-plot of TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train,features_test), axis=0)\n",
    "labels = np.concatenate((labels_train,labels_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dim_red(model, features, labels, n_components=2):\n",
    "    \n",
    "    # Creation of the model\n",
    "    if (model == 'PCA'):\n",
    "        mod = PCA(n_components=n_components)\n",
    "        title = \"PCA decomposition of top 5000 features\"  # for the plot\n",
    "        \n",
    "    elif (model == 'TSNE'):\n",
    "        mod = TSNE(n_components=2)\n",
    "        title = \"t-SNE decomposition\" \n",
    "\n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "    # Fit and transform the features\n",
    "    principal_components = mod.fit_transform(features)\n",
    "    \n",
    "    # Put them into a dataframe\n",
    "    df_features = pd.DataFrame(data=principal_components,\n",
    "                     columns=['PC1', 'PC2'])\n",
    "    \n",
    "    # Now we have to paste each row's label and its meaning\n",
    "    # Convert labels array to df\n",
    "    df_labels = pd.DataFrame(data=labels,\n",
    "                             columns=['label'])\n",
    "    \n",
    "    df_full = pd.concat([df_features, df_labels], axis=1)\n",
    "    df_full['label'] = df_full['label'].astype(str)\n",
    "\n",
    "    # Get labels name\n",
    "    category_names = {\n",
    "        \"1\": 'Funded loans',\n",
    "        \"2\": 'Expired loans'\n",
    "    }\n",
    "\n",
    "    # And map labels\n",
    "    df_full['label_name'] = df_full['label']\n",
    "    df_full = df_full.replace({'label_name':category_names})\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.scatterplot(x='PC1',\n",
    "                    y='PC2',\n",
    "                    hue=\"label_name\", \n",
    "                    data=df_full,\n",
    "                    palette=[\"red\",\"blue\"],\n",
    "                    alpha=.7).set_title(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dim_red(\"PCA\", \n",
    "             features=features, \n",
    "             labels=labels,\n",
    "             n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top n TF-IDF scores for each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = top_feats_by_class(features_train2, labels_train, tfidf.get_feature_names(), min_tfidf=0.1, top_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdB_Z3ypC1EL"
   },
   "source": [
    "# Logistic regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we fit a logistic regression model on the tf-idf features. By adjusting the C parameter we obtain a test set accuracy of 76%. Next, we construct a confusion matrix and plot the most important words for each bin. Finally, we have a look at some misclassified descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3876
    },
    "colab_type": "code",
    "id": "1hUiArY3K44p",
    "outputId": "748e72d7-d46e-49a3-a88d-7ae86610802f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=2, solver='sag')\n",
    "logreg = logreg.fit(features_train, labels_train)\n",
    "y_pred = logreg.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hU6HqstkLNCR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_test))\n",
    "print(classification_report(labels_test, y_pred,target_names=['bin 1','bin 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix + ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "aux_df = combined[['status', 'status_Code']].drop_duplicates().sort_values('status_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, y_pred)\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['status'].values, \n",
    "            yticklabels=aux_df['status'].values,\n",
    "            cmap=\"Blues\", fmt='g')\n",
    "ax.set_ylim([0,2])\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = logreg.predict_proba(features_test)\n",
    "preds = probs[:,0]\n",
    "y_true = labels_test.values\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, preds, pos_label=1)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important words for each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Five most important words to classify funded loans: {she; widow; toilet; machine; tuition}\n",
    "    * Five most important words to classify expired loans: {his; he; clothe; store; products}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n",
    "importance = get_most_important_features(tfidf, logreg, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Funded loans', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Expired loans', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassified descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_names = {\n",
    "    1: 'funded',\n",
    "    2: 'expired'\n",
    "}\n",
    "\n",
    "# Indexes of the test set\n",
    "index_X_test = X_test.index\n",
    "\n",
    "# We get them from the original df\n",
    "df_test = combined.loc[index_X_test]\n",
    "\n",
    "# Add the predictions\n",
    "df_test['Prediction'] = y_pred\n",
    "\n",
    "# Clean columns\n",
    "df_test = df_test[['description_ENG_Parsed_8','description_ENG', 'status', 'status_Code', 'Prediction','funded_amount','loan_amount']]\n",
    "\n",
    "# Decode\n",
    "df_test['status_Predicted'] = df_test['Prediction']\n",
    "df_test = df_test.replace({'status_Predicted':status_names})\n",
    "\n",
    "# Clean columns again\n",
    "df_test = df_test[['description_ENG_Parsed_8','description_ENG', 'status', 'status_Predicted','funded_amount','loan_amount']]\n",
    "\n",
    "condition = (df_test['status'] != df_test['status_Predicted'])\n",
    "\n",
    "df_misclassified = df_test[condition]\n",
    "\n",
    "def output_article(row_article):\n",
    "    print('Actual status: %s' %(row_article['status']))\n",
    "    print('Predicted status: %s' %(row_article['status_Predicted']))\n",
    "    print('-------------------------------------------')\n",
    "    print('Text: ')\n",
    "    print('%s' %(row_article['description_ENG_Parsed_8']))\n",
    "    print('%s' %(row_article['description_ENG']))\n",
    "    \n",
    "import random\n",
    "random.seed(8)\n",
    "list_samples = random.sample(list(df_misclassified.index), 3)\n",
    "\n",
    "output_article(df_misclassified.loc[list_samples[0]])\n",
    "output_article(df_misclassified.loc[list_samples[1]])\n",
    "output_article(df_misclassified.loc[list_samples[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# C\n",
    "C = [float(x) for x in np.linspace(start = 1, stop = 3, num = 10)]\n",
    "\n",
    "# solver\n",
    "solver = ['newton-cg', 'sag', 'saga', 'lbfgs']\n",
    "\n",
    "# class_weight\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "# penalty\n",
    "penalty = ['l2']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C,\n",
    "               'solver': solver,\n",
    "               'class_weight': class_weight,\n",
    "               'penalty': penalty}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=logreg,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=5,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "C = [float(x) for x in np.linspace(start = 1.8, stop = 2.2, num = 5)]\n",
    "solver = ['sag']\n",
    "class_weight = ['None']\n",
    "penalty = ['l2']\n",
    "\n",
    "param_grid = {'C': C,\n",
    "               'solver': solver,\n",
    "               'class_weight': class_weight,\n",
    "               'penalty': penalty}\n",
    "\n",
    "# Create a base model\n",
    "lrc = LogisticRegression(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=lrc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lrc = grid_search.best_estimator_\n",
    "lrc_pred = best_lrc.predict(features_test)\n",
    "\n",
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_lrc.predict(features_train)))\n",
    "\n",
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, best_lrc.predict(features_test)))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,lrc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB(alpha=0.1)\n",
    "Naive.fit(features_train,labels_train)# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(features_test)# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, labels_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVC\n",
    "# fit the training dataset on the classifier\n",
    "SVC = LinearSVC(random_state=0, tol=1e-5, C=0.7, loss='hinge')\n",
    "SVC.fit(features_train,labels_train)# predict the labels on validation dataset\n",
    "predictions_SVC = SVC.predict(features_test)# Use accuracy_score function to get the accuracy\n",
    "print(\"SVC Accuracy Score -> \",accuracy_score(predictions_SVC, labels_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check gender bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the logistic regression model suggest that a loan will be fully funded when the words 'her' and 'she' are mentioned while a loan will be expired when the words 'he' and 'his' are mentioned. This should be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['gender_reclassified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.groupby(['gender_reclassified', 'status']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of target_variable vs gender\n",
    "target_gender = pd.crosstab(index=combined[\"gender_reclassified\"], \n",
    "                            columns=combined[\"status\"],\n",
    "                             margins=True)   # Include row and column totals\n",
    "target_gender.columns = [\"expired\",\"funded\",\"coltotal\"]\n",
    "target_gender.index= [\"female\",\"male\",\"rowtotal\"]\n",
    "target_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gender_proportions=target_gender.div(target_gender[\"coltotal\"],\n",
    "                   axis=0)\n",
    "target_gender_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gender_proportions=target_gender_proportions.loc[['female', 'male'], :'expired']\n",
    "target_gender_proportions.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fastText and tf-idf TDS post.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
